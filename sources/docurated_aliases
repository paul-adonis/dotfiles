# ============ Docurated Aliases =============
DOC_HOME='/Users/sampaul/Development/Docurated'
alias doc="cd $DOC_HOME"


# ===== JIRA ZSH Plugin / URLs ========
JIRA_URL='https://docurated.atlassian.net'
GITHUB_URL='https://github.com/Docurated'
JENKINS_URL='https://jenkins.service.consul'
HANGOUTS_URL='https://hangouts.google.com/hangouts'


# ===== GOOGLE HANGOUTS ========
openHangout() { open "$HANGOUTS_URL/_/docurated.com/$1?authuser=$2" }
alias relho='openHangout relevance sam@docurated.com'
alias relno='open "https://docs.google.com/a/docurated.com/document/d/1ktuCdngDhUZxfd5-uND9jC23_l9SzQiCkLMC2RU0zzg/edit?usp=sharing"'


# ===== GIT REPOS ========
alias docweb="cd $DOC_HOME/website/rails"
alias docutil="cd $DOC_HOME/utilities"
alias doccli="cd $DOC_HOME/clients"
alias docserv="cd $DOC_HOME/services"
alias docevents="cd $DOC_HOME/events"
alias docact="cd $DOC_HOME/activity"
alias dockube="cd $DOC_HOME/kubernetes"

alias hubc='open $GITHUB_URL/clients'
alias hubw='open $GITHUB_URL/website'
alias hubu='open $GITHUB_URL/utilities'
alias hubs='open $GITHUB_URL/services'
alias hube='open $GITHUB_URL/events'
alias huba='open $GITHUB_URL/activity'


# ===== JENKINS ========
alias jenk='open $JENKINS_URL/'
alias jenkc='open $JENKINS_URL/job/Client%20Build/'
alias jenks='open $JENKINS_URL/job/BuildTestDeployWebStage/'
alias jenkw='open $JENKINS_URL/job/BuildTestDeployWebMaster/'
alias jenke='open $JENKINS_URL/job/Events%20Build/'
alias jenka='open $JENKINS_URL/job/Activity/'


# ===== CONSUL ========
alias ctoken='echo $CONSUL_TOKEN | pbcopy'
alias conpget='consul kv get -http-addr=consul.service.consul:8500 -datacenter=us-east-1-production'
alias conpput='consul kv put -token=$CONSUL_TOKEN -http-addr=consul.service.consul:8500 -datacenter=us-east-1-production'
alias condget='consul kv get -http-addr=consul.service.consul:8500 -datacenter=us-east-1-demo'
alias condput='consul kv put -token=$CONSUL_TOKEN -http-addr=consul.service.consul:8500 -datacenter=us-east-1-demo'
alias conbget='consul kv get -http-addr=consul.service.consul:8500 -datacenter=us-west-2-biomarin'
alias conbput='consul kv put -token=$CONSUL_TOKEN -http-addr=consul.service.consul:8500 -datacenter=us-west-2-biomarin'

alias conpgetana='conpget repos/analytics/current_release'
alias condgetana='condget repos/analytics/current_release'
alias conbgetana='conbget repos/analytics/current_release'

alias conpputana='conpput repos/analytics/current_release'
alias condputana='condput repos/analytics/current_release'
alias conbputana='conbput repos/analytics/current_release'


# ===== SSH ========
sshconnect() { ssh $1.$2.$3 }

alias cdweb='sshconnect demo web'
alias cpweb='sshconnect production web'

alias cdapi='sshconnect demo api'
alias cpapi='sshconnect production api'

alias cdrapp='docutil && $DOC_HOME/utilities/bin/ssh-first demo resque'
alias cprapp='docutil && $DOC_HOME/utilities/bin/ssh-first production resque'

alias cdact='ssh demo.activity.0'
alias cpact='ssh production.activity.0'
alias cbact='ssh biomarin.activity.0'

alias cdana='ssh demo.analytics.0'
alias cpana='ssh production.analytics.0'
alias cbana='ssh biomarin.analytics.0'


# ===== KUBE ========
alias kcont='kubectl config use-context'
alias kpods='kubectl get pods'
alias kjobs='kubectl get jobs'

kube_pod_grep() {
    kubectl get pods | grep $1 | awk '{print $1}'
}
alias kpodg="kube_pod_grep"

kube_connect() {
    kubectl exec -it $1 bash
}
alias ck="kube_connect"

kube_grep_connect() {
    ck `kpodg $1`
}
alias ckg="kube_grep_connect"
alias ckairweb="ckg airflow-webserver"
alias ckairwork="ckg airflow-worker"
alias ckairsched="ckg airflow-scheduler"
alias ckrails="$DOC_HOME/kubernetes/bin/console"

kube_deploy() {
    $DOC_HOME/kubernetes/bin/deploy $1 v$2
}
alias kdair='kube_deploy airflow'


# tunneling ports
MAIN_PG_PORT_DEMO=5433
MAIN_PG_PORT_PROD=5434
MAIN_PG_PORT_BIO=5435

ACT_PG_PORT_DEMO=5436
ACT_PG_PORT_PROD=5437
ACT_PG_PORT_BIO=5438

ANA_PG_PORT_DEMO=5439
ANA_PG_PORT_PROD=5440
ANA_PG_PORT_BIO=5441

AIR_PG_PORT_DEMO=5442
AIR_PG_PORT_PROD=5443

ACT_RESQ_PORT_DEMO=8286
ACT_RESQ_PORT_PROD=8287
ACT_RESQ_PORT_BIO=8288

REDIS_PORT_DEMO=6380

ACT_PG_HOST_DEMO='activity-demo.cpk5zbekyq7f.us-east-1.rds.amazonaws.com'
ACT_PG_HOST_PROD='activity-production.cpk5zbekyq7f.us-east-1.rds.amazonaws.com'
ACT_PG_HOST_BIO='activity-biomarin.cr9ikdrfc2gy.us-west-2.rds.amazonaws.com'

ANA_PG_HOST_DEMO='analytics-demo.cpk5zbekyq7f.us-east-1.rds.amazonaws.com'
ANA_PG_HOST_PROD='analytics-production.cpk5zbekyq7f.us-east-1.rds.amazonaws.com'
ANA_PG_HOST_BIO='analytics-biomarin.cr9ikdrfc2gy.us-west-2.rds.amazonaws.com'

AIR_PG_HOST_DEMO='airflow-demo.cpk5zbekyq7f.us-east-1.rds.amazonaws.com'
AIR_PG_HOST_PROD='airflow-production.cpk5zbekyq7f.us-east-1.rds.amazonaws.com'
AIR_PG_HOST_BIO='airflow-biomarin.cr9ikdrfc2gy.us-west-2.rds.amazonaws.com'


# ssh remote <user@hostname> <local port>:<host>:<port>
tunnel_port() { ssh -nNT $1 -L ${2}:${3}:${4} }
alias killtunneldemo="pkill -f 'nNT demo'"
tunneldemo() {
    killtunneldemo
    tunnel_port demo.postgres.0 $MAIN_PG_PORT_DEMO localhost 5432 &
    tunnel_port demo.analytics.0 $ANA_PG_PORT_DEMO $ANA_PG_HOST_DEMO 5432 &
    tunnel_port demo.analytics.0 $AIR_PG_PORT_DEMO $AIR_PG_HOST_DEMO 5432 &
    tunnel_port demo.analytics.0 $REDIS_PORT_DEMO main.redis.service.consul 6379 &
}
alias killtunnelprod="pkill -f 'nNT prod'"
tunnelprod() {
    killtunnelprod
    tunnel_port production.postgres.0 $MAIN_PG_PORT_PROD localhost 5432 &
    tunnel_port production.analytics.0 $ANA_PG_PORT_PROD $ANA_PG_HOST_PROD 5432 &
    tunnel_port production.analytics.0 $AIR_PG_PORT_PROD $AIR_PG_HOST_PROD 5432 &
}
alias killtunnelbio="pkill -f 'nNT biomarin'"
tunnelbio() {
    killtunnelbio
    tunnel_port biomarin.analytics.0 $MAIN_PG_PORT_BIO biomarin.postgres.0 5432 &
    tunnel_port biomarin.analytics.0 $ANA_PG_PORT_BIO $ANA_PG_HOST_BIO 5432 &
    tunnel_port biomarin.analytics.0 $AIR_PG_PORT_BIO $AIR_PG_HOST_BIO 5432 &
}

alias pql='psql -d docurated -p 5432'

connect_pql() { psql -w -h localhost -p ${1} -U ${2} ${3} }
alias dpql='connect_pql $MAIN_PG_PORT_DEMO rails docurated'
alias ppql='connect_pql $MAIN_PG_PORT_PROD rails docurated'
alias bpql='connect_pql $MAIN_PG_PORT_BIO rails docurated'

alias dpqlact='connect_pql $ACT_PG_PORT_DEMO activitydemo activity'
alias ppqlact='connect_pql $ACT_PG_PORT_PROD activityproduction activity'
alias bpqlact='connect_pql $ACT_PG_PORT_BIO activitybiomarin activity'

alias dpqlana='connect_pql $ANA_PG_PORT_DEMO analyticsdemo analytics'
alias ppqlana='connect_pql $ANA_PG_PORT_PROD analyticsproduction analytics'
alias bpqlana='connect_pql $ANA_PG_PORT_BIO analyticsbiomarin analytics'

alias dpqlair='connect_pql $AIR_PG_PORT_DEMO airflowdemo airflow'
alias ppqlair='connect_pql $AIR_PG_PORT_PROD airflowproduction airflow'

alias cadmin='sshconnect production admin'

# reset the stupid mac osx DNS for tunnelblick to work
# alias resetdns='sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist && sudo launchctl load -w /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist'
alias resetdns='sudo killall -HUP mDNSResponder'


# ===== AIRFLOW ========
alias airw='airflow webserver -D &'
alias killairw='pkill -9 airflow.webserver'
alias airwr='killairw && airw'
alias airc='airflow clear'
alias airb='airflow backfill -s `date +%Y-%m-%d`'
alias airl='airflow list_dags'

airflow_full_clear() {
sql="DELETE FROM dag_run WHERE dag_id = '$3';
DELETE FROM task_instance WHERE dag_id = '$3';
DELETE FROM task_fail WHERE dag_id = '$3';"
    psql -c "$sql" -w -h localhost -p $1 -U $2 airflow
}
alias aircc='airflow_full_clear 5432 sampaul'
alias airdcc='airflow_full_clear $AIR_PG_PORT_DEMO airflowdemo'
alias airpcc='airflow_full_clear $AIR_PG_PORT_PROD airflowproduction'
airccb() {
    aircc $1
    airb $1
}

airflow_backfill_catchup() {
sql="INSERT INTO dag_run 
(dag_id, execution_date, state, run_id, external_trigger)
VALUES
('$3', '${4:-`date +%Y-%m-%d`}', 'success', 'catching_up_$3', 'f');"
    psql -c "$sql" -w -h localhost -p $1 -U $2 airflow
}
alias airbb='airflow_backfill_catchup 5432 sampaul'
alias airdbb='airflow_backfill_catchup $AIR_PG_PORT_DEMO airflowdemo'
alias airpbb='airflow_backfill_catchup $AIR_PG_PORT_PROD airflowproduction'

airflow_new_connection() {
sql="INSERT INTO connection 
(conn_id, conn_type, host, schema, login, password, port)
VALUES
('$3', 'postgres', 'localhost', '$4', '$5', '$6', ${7:-5432});"
    psql -c "$sql" -w -h localhost -p $1 -U $2 airflow
}
alias airconn='airflow_new_connection 5432 sampaul'

airflow_status() {
    wc="AND ${4:-1=1}"
read -r -d '' q <<- EOM
    SELECT 
        execution_date::timestamp(0), 
        task_id, state, 
        start_date::timestamp(0), 
        end_date::timestamp(0), 
        duration, pid
    FROM task_instance
    WHERE dag_id = '${3}'
    AND execution_date >= (
        SELECT MAX(execution_date) 
        FROM dag_run 
        WHERE dag_id = '${3}'
    )
    ${wc}
    ORDER BY state
EOM
    psql -c "$q" -w -h localhost -p $1 -U $2 airflow
}
alias airstat='airflow_status 5432 sampaul'
alias airdstat='airflow_status $AIR_PG_PORT_DEMO airflowdemo'
alias airpstat='airflow_status $AIR_PG_PORT_PROD airflowproduction'

airflow_tail() {
    latest=`ls -t $AIRFLOW_HOME/logs/$1/$2 | head -1`
    tail -f -n 200 $AIRFLOW_HOME/logs/$1/$2/$latest
}
alias airtail='airflow_tail'


# ===== ARCANIST  ========
alias arcl='arc list'
alias arcb='arc browse'
alias arcd='git pull && arc diff --reviewers'
arcdiff() {
    git pull && arc diff --reviewers $1 --message ""
}

arcPatchMerge() {
    arc patch $1
    gco master
    arc land arcpatch-$1
    git pull
}
alias arcpm='arcPatchMerge'


# ===== SPARK ========
alias emr='aws emr list-clusters --active --query "Clusters[*].{Name:Name,ID:Id,State:Status.State,Info:Status.StateChangeReason.Message}"'

SPARK_KEY_PAIR='$HOME/.ssh/sams-other-key-pair.pem'
ssh_spark() {
    # printf "Using cluster DNS: %s\n" "${SPARK_PROD_CLUSTER_DNS:?You must set SPARK_PROD_CLUSTER_DNS}"
    ssh -i $1 -L 8087:$2:18080 -L 8086:$2:8088 hadoop@$2
}
alias cpspark='ssh_spark ~/.ssh/sams-other-key-pair.pem $SPARK_PROD_CLUSTER_DNS'
alias cdspark='ssh_spark ~/.ssh/sams-other-key-pair.pem $SPARK_DEMO_CLUSTER_DNS'

add_spark_step() {
    printf "Using cluster ID: %s\n" "${SPARK_PROD_CLUSTER_ID:?You must set SPARK_PROD_CLUSTER_ID}"
    aws emr add-steps --cluster-id $1 --steps "Type=spark,Name='$3',Args=[--deploy-mode,cluster,--master,yarn,--conf,spark.executorEnv.ENV=production,--conf,spark.yarn.appMasterEnv.ENV=production,--conf,spark.speculation=true,$2]"
}
alias spspark='add_spark_step $SPARK_PROD_CLUSTER_ID s3://docurated-production-analytics/main.py'
alias sdspark='add_spark_step $SPARK_DEMO_CLUSTER_ID s3://docurated-demo-analytics/main.py'

set_spark_env_var() { export $1=$2 }
alias sparkpc='set_spark_env_var SPARK_PROD_CLUSTER_ID'
alias sparkdc='set_spark_env_var SPARK_DEMO_CLUSTER_ID'
alias sparkpd='set_spark_env_var SPARK_PROD_CLUSTER_DNS'
alias sparkdd='set_spark_env_var SPARK_DEMO_CLUSTER_DNS'

cp_to_spark_s3() { aws s3 cp $2 s3://docurated-$1-analytics/$2 }
alias s3sparkpcp='cp_to_spark_s3 production'
alias s3sparkdcp='cp_to_spark_s3 demo'


# ===== CLIENT APP ========
setDemoVersion() {
    cd $DOC_HOME/clients/deploy
    rm client_demo_version.txt
    touch client_demo_version.txt
    echo $1 > client_demo_version.txt
    s3cmd put client_demo_version.txt s3://docurated-private/client_demo_version.txt
}

alias pd='doccli && python docurated.py'
alias pyserv='doccli && python -m SimpleHTTPServer'
alias coffserv='doccli && cd browser && coffee --watch --compile --output lib/ src/'
# alias cliserv='doccli && pyserv'
# alias cliserv='doccli && cd browser && coffserv'
alias clogd='doccli && tail -f app.log'
alias clog='tail -f ~/.docurated/app.log'
alias mount_vol='sudo mount -t smbfs //Guest@50.17.45.211/vol /private/nfs'
alias msd='sudo mount -t smbfs //Guest@50.17.45.211/vol /private/nfs'


# ===== RAILS ========
alias rs='rails s'
alias rc='rails c'
alias rsp='rails s -p '
alias rcd='docweb && rails c development'
alias rsd='docweb && thin start --ssl --ssl-verify --ssl-key-file ~/Development/server.key --ssl-cert-file ~/Development/server.crt'
alias wlog='docweb && tail -f log/development.log'
alias killrails="pkill -f 'rails s'"
alias restartrails="killrails && docweb && rails s -d"


# ===== SOLR ========
alias solr_start='/usr/local/opt/solr/bin/solr start -c'
alias solr_stop='/usr/local/opt/solr/bin/solr stop -all'
alias restartsolr='solr_stop && solr_start'
alias testsolrs='docweb && bundle exec sunspot-solr start -p 8984'
alias testsolrr='docweb && bundle exec sunspot-solr run -p 8984'


# ===== SERVICES ========
alias serv_topo="cd $DOC_HOME/services/javaworker && sbt topology/run"
alias serv_master="cd $DOC_HOME/services/javaworker && sbt master/run"


# ===== WORKERS ========
alias resqd='open https://demosecure.docurated.com/site/resque/overview'
alias resqp='open https://admin.docurated.com/site/resque/overview'

alias start_resqs='docweb && resque-scheduler --environment development &'
alias start_resqp='docweb && resque-pool --environment development &'
alias start_sidekiq='docweb && RAILS_ENV=development sh bin/sidekiq.sh start &'

alias stop_resqs='pkill -f "resque-scheduler"'
alias stop_resqp='pkill -f "resque-pool-master"'
alias stop_sidekiq='docweb && RAILS_ENV=development sh bin/sidekiq.sh stop'

alias start_workers='start_resqs; start_resqp; start_sidekiq'
alias stop_workers='stop_resqs; stop_resqp; stop_sidekiq'

alias start_dynamo='cd ~/dynamo && nohup java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -inMemory -port 8888 > ~/dynamoout.txt 2> ~/dynamoerr.text < /dev/null &'

alias start_all='pstarts; testsolrs; start_dynamo; solrs &'
alias start_cli='pyserv &; coffserv &; doccli; mount_vol'

alias dresq='docutil && docformation/docconnection.rb demo.railsapp "ps -ef f | grep resque"'
alias sresq='docutil && docformation/docconnection.rb stage.railsapp "ps -ef f | grep resque"'


# ===== SCRATCH/UNUSED ========
searchgit() { open "https://github.com/Docurated/$1/search?utf8=✓&q=$2" }
alias hubcs='searchgit clients'
alias hubws='searchgit website'
alias hubus='searchgit utilities'
alias hubss='searchgit services'
alias hubes='searchgit events'
alias hubas='searchgit activity'


ENVS=('DEMO' 'PROD' 'BIO')
HOST_TYPES=('MAIN' 'ACT' 'ANA' 'AIR')
SERVICES=('PG' 'RESQ')

tunnel_ports() {
    str=""
    for e in $ENVS; do
        for h in $HOST_TYPES; do
            port="${h}_PG_PORT_${e}"
            post="${h}_PG_HOST_${e}"
            echo port: ${(P)port}
            echo host: ${(P)host}
            str+=${(P)port}
            # echo port: ${!port}
        done
    done
    echo $str
}

